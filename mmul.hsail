module &m:1:0:$full:$large:$default;

prog kernel &mmul_kernel(
	kernarg_u64 %a,
	kernarg_u64 %b,
	kernarg_u64 %c,
	kernarg_u64 %arows,
	kernarg_u64 %acols)
{
	// BB#0:
	ld_kernarg_align(8)_width(all)_u64	$d0, [%arows];

	// arows == 0 -> skip outer loop
	cmp_eq_b1_s64	$c0, $d0, 0;
	cbr_b1	$c0, @BB0_5;
	// BB#1:                                // %.preheader.lr.ph
	ld_kernarg_align(8)_width(all)_u64	$d1, [%acols];
	ld_kernarg_align(8)_width(all)_u64	$d2, [%c];
	ld_kernarg_align(8)_width(all)_u64	$d3, [%b];
	ld_kernarg_align(8)_width(all)_u64	$d4, [%a];
	workitemabsid_u32	$s0, 0;
	cvt_u64_u32	$d5, $s0; // $d5 = i
	mul_u64	$d6, $d5, $d0; // $d6 = i * arows
	mul_u64	$d5, $d5, $d1; // $d5 = i * acols
	mov_b32	$s0, 0; // 
	mov_b64	$d7, 0; // $d7 = k
	mov_b32	$s1, 1;
	mov_b64	$d8, 0;
	mov_b64	$d9, $d7; // $d9 = j = k = 0

@BB0_2:
    // acols == 0 -> skip inner loop 
	// %.preheader
	cmp_eq_b1_s64	$c0, $d1, 0;
	mov_b32	$s2, $s1;
	mov_b64	$d11, $d7; // $d11 = k = 0
	mov_b64	$d10, $d8;
	cbr_b1	$c0, @BB0_4;

@BB0_3:
	// c_ij += a[k*arows + j] * b[i * acols + k];
	// %.lr.ph
	// $d12 = a[k*arows + j]
	mad_u64	$d12, $d11, $d0, $d9; // $d12 = k * arows + j
	shl_u64	$d12, $d12, 3; // $d12 = (k * arows + j) * 8
	add_u64	$d12, $d4, $d12;
	ld_global_align(8)_const_width(all)_f64	$d12, [$d12];

	// $d11 = b[i*acols + k]
	add_u64	$d11, $d11, $d5; // $d11 = k * acols
	shl_u64	$d11, $d11, 3;
	add_u64	$d11, $d3, $d11; 
	ld_global_align(8)_f64	$d11, [$d11];

    // $d11 = a[k*arows + j] * b[i*acols + k] 
	mul_f64	$d11, $d12, $d11;

	// c_ij += ...
	add_f64	$d10, $d10, $d11;

	// $s3 = k + 1
	add_u32	$s3, $s2, 1;
	cvt_u64_u32	$d11, $s2;
	cmp_lt_b1_u64	$c0, $d11, $d1;
	mov_b32	$s2, $s3;
	cbr_b1	$c0, @BB0_3;

@BB0_4:
    // c[i*arows + j] = c_ij
	// %._crit_edge
	add_u64	$d9, $d9, $d6;
	shl_u64	$d9, $d9, 3;
	add_u64	$d9, $d2, $d9;
	st_global_align(8)_f64	$d10, [$d9];
	add_u32	$s0, $s0, 1;
	cvt_u64_u32	$d9, $s0;
	cmp_lt_b1_u64	$c0, $d9, $d0;
	cbr_b1	$c0, @BB0_2;

@BB0_5:
	// %._crit_edge4
	ret;
};
